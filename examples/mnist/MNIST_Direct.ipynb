{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "MNIST Direct\n",
    "============\n",
    "\n",
    "This walk-through demonstrates the flexiblity of ngraph and how users can build the MNIST multi-layer perceptron model using ngraph components only. **For a simpler example, see the `mnist_mlp.py` example that uses the neon frontend to build the identical model.**\n",
    "\n",
    "Note: here we do use neon components for downloading the MNIST data and providing minibatches of data to the ngraph model using neon's `ArrayIterator` object. However, the model definition and training is done with the ngraph API directly.\n",
    "\n",
    "MNIST is a computer vision dataset consisting of 70,000 images of handwritten digits. Each image has 28x28 pixels for a total of 784 features, and is associated with a digit between 0-9.\n",
    "\n",
    "<img src=\"http://corpocrat.com/wp-content/uploads/2014/10/figure_1.png\" width=200px>\n",
    "\n",
    "Setup data and axes\n",
    "-------------------\n",
    "\n",
    "We first use neon components to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from ngraph.frontends.neon import ArrayIterator, NgraphArgparser\n",
    "from ngraph.frontends.neon import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "batch_size = 128\n",
    "\n",
    "# Create the dataloader\n",
    "train_data, valid_data = MNIST('~/nervana/data/').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function automatically splits the images `X` and labels `y` into training `(60,000 examples)` and testing `(10,000 examples)` data. The training images `train_data` is a numpy array with shape `(num_examples, num_features) = (60000, 1, 28, 28)`.\n",
    "During training, neon iterates over the training examples to compute the gradients. We use the following commands to set up the `ArrayIterator` object that handles sending data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set = ArrayIterator(train_data, batch_size, total_iterations=2000)\n",
    "valid_set = ArrayIterator(valid_data, batch_size)\n",
    "\n",
    "num_batches = np.floor(train_set.ndata/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we create the axes needed by the model. There are several important axes here:\n",
    "- (`C`, `H`, `W`, `N`): the shape of the input data in (channels, height, width, batch_size)\n",
    "- (`M`): the number of hidden units in our affine layer.\n",
    "- (`Y`): the shape of the output of the model, which is equal to the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "\n",
    "N = ng.make_axis(name='N', docstring=\"minibatch size\")\n",
    "C = ng.make_axis(docstring=\"channels\")\n",
    "H = ng.make_axis(docstring=\"image height\")\n",
    "W = ng.make_axis(docstring=\"image width\")\n",
    "Y = ng.make_axis(docstring=\"target shape\")\n",
    "\n",
    "M = ng.make_axis(100, docstring=\"hidden layer nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From our input data, we can already set the length of many of these axes. Because `M` is user defined, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "C.length, H.length, W.length = train_set.shapes['image']\n",
    "N.length = batch_size\n",
    "Y.length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can then create the needed placeholders and variables for defining the model:\n",
    "- `x`: placeholder for the input image\n",
    "- `t`: placeholder for the input target\n",
    "- `w1`: weight matrix between the input image and the hidden layer.\n",
    "- `w2`: weight matrix between the hidden layer and the output layer.\n",
    "\n",
    "Defining the placeholders is relatively straightforward. Note that the ordering for `x` matters since the data from `ArrayIterator` is provided in the `CHWN` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = ng.placeholder([C, H, W, N])\n",
    "t = ng.placeholder([N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define the weights as `variables` and initialize them as random gaussian variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1_axes = ng.make_axes([M, C, H, W])\n",
    "w1 = ng.variable(initial_value=np.random.normal(0.0, 0.01, w1_axes.lengths), axes=w1_axes)\n",
    "\n",
    "w2_axes = ng.make_axes([Y, M])\n",
    "w2 = ng.variable(initial_value=np.random.normal(0.0, 0.01, w2_axes.lengths), axes=w2_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Network\n",
    "-------\n",
    "\n",
    "We then define our network using the ngraph operations to obtain the activations from the hidden layer `h1` and the output layer `h2`.\n",
    "\n",
    "The hidden layer is expressed as a Linear layer followed by a rectified linear activation function. The output layer is also linear, but with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "h1 = ng.maximum(ng.dot(w1, x / 255.), 0)\n",
    "h2 = ng.sigmoid(ng.dot(w2, h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cost\n",
    "----\n",
    "\n",
    "To train the network, we use cross entropy binary loss function. We also define some useful computations for obtaining the total loss, the mean loss, predictions, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = ng.cross_entropy_binary(h2, ng.one_hot(t, axis=Y))\n",
    "\n",
    "# useful model operations\n",
    "total_loss = ng.sum(loss, out_axes=())\n",
    "mean_cost = ng.mean(loss, out_axes=())\n",
    "predictions = ng.argmax(h2)\n",
    "errors = ng.not_equal(predictions, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optimizer\n",
    "---------\n",
    "\n",
    "Here we implement gradient descent with momentum using the ngraph API. Just as with the Logistic Regression example, we use `ng.deriv` to obtain the gradients for all the variables that we with to optimize over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "variables = list(total_loss.variables())\n",
    "grads = [ng.deriv(total_loss, variable) / N.length for variable in variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the SGD with momentum optimizer, we implement below the required steps to:\n",
    "1. define the velocities for each variable as `persistent_tensor`.\n",
    "2. update the velocities with each call according to $v' = v m - \\alpha \\nabla$\n",
    "3. update the parameters via $W' = W + v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "with ng.Op.saved_user_deps():\n",
    "    velocities = [ng.persistent_tensor(axes=variable.axes, initial_value=0.)\n",
    "                  for variable in variables]\n",
    "    velocity_updates = [ng.assign(velocity,\n",
    "                                  velocity * momentum - learning_rate * grad)\n",
    "                        for velocity, grad in zip(velocities, grads)]\n",
    "    param_updates = [ng.assign(variable, variable + velocity)\n",
    "                     for variable, velocity in zip(variables, velocities)]\n",
    "\n",
    "updates = ng.doall(velocity_updates + param_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Several of the ops above are defined with the `ng.Op.saved_user_deps()` context. Ops defined in this context are excluded as depend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define two computations below, one for updating the weights and returning the average cost for each minibatch of data. The second computation obtains the misclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformer = ngt.make_transformer()\n",
    "train_comp = transformer.computation([mean_cost, updates], x, t)\n",
    "error_comp = transformer.computation(errors, x, t)\n",
    "transformer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "Finally, we can train the model by iterating over the training set, calling `train_comp` for each minibatch of data. We also print the total cost at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_cost = []\n",
    "for mb_idx, npbufs in enumerate(train_set):\n",
    "    batch_cost, _ = train_comp(npbufs['image'], npbufs['label'])\n",
    "    total_cost.append(float(batch_cost))\n",
    "\n",
    "    if len(total_cost) == num_batches:\n",
    "        print(\"[Epoch %s] Cost = %s\" % ((mb_idx + 1) // num_batches, np.mean(total_cost)))\n",
    "        total_cost = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Validation\n",
    "----------\n",
    "\n",
    "Last, we take our trained graph, and evaluate the misclassification error on a held-out `valid_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "running_error = 0.\n",
    "all_errors = []\n",
    "valid_set.reset()\n",
    "\n",
    "\n",
    "for npbufs in valid_set:\n",
    "    error_val = error_comp(npbufs['image'], npbufs['label'])\n",
    "    all_errors.append(list(error_val))\n",
    "\n",
    "all_errors = all_errors[:valid_set.ndata]  # Truncate to remove any leftovers\n",
    "\n",
    "print('Validation Error = %s%%' % (np.mean(all_errors) * 100.,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
